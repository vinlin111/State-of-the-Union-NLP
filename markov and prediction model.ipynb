{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the code was done through spyder because we could not install the keras \n",
    "and tensorflow packages as well as many others in jupyter.\n",
    "these were necessary when running the language model. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in libraries\n",
    "import itertools\n",
    "import random\n",
    "from random import sample\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import pickle\n",
    "import nltk \n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import load\n",
    "from random import randint\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow \n",
    "import keras \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string manipulation and creating the pandas dataframe\n",
    "def txt(title):\n",
    "    title = title[6:]\n",
    "    title = title[:-4]\n",
    "    title = re.sub(\"_\", \" \", title)\n",
    "    title = re.sub(\"/\", \" \", title)\n",
    "    return title\n",
    "\n",
    "all_files = os.listdir(\"state_of_the_unions/\")\n",
    "correct_filenames = []\n",
    "for item in sorted(all_files):\n",
    "    correct_filenames.append(\"state_of_the_unions/\" + item)\n",
    "    \n",
    "files = {}\n",
    "\n",
    "for filename in correct_filenames:\n",
    "    with open(filename, \"r\") as file:\n",
    "        if filename in files:\n",
    "            continue\n",
    "        files[filename] = file.read()\n",
    "\n",
    "df = pd.DataFrame(files.items(), columns=[\"pres_sotu\", \"speech\"])\n",
    "tokens = []\n",
    "\n",
    "for row in df[\"speech\"]:\n",
    "    row = row.lower()\n",
    "    sotu = re.split('[^a-zA-Z0-9]', row)\n",
    "    clean_sotu = list(filter(None,sotu))\n",
    "    tokens.append(clean_sotu)\n",
    "  \n",
    "\n",
    "president_name = []\n",
    "year = []\n",
    "\n",
    "for row in df['pres_sotu'].apply(txt):\n",
    "    row = row.split(\" \")\n",
    "    president_name.append(row[-2])\n",
    "    year.append(row[-1])\n",
    "    \n",
    "df[\"president\"] = president_name\n",
    "df[\"year\"] = year\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Start of Language Model Prediction\n",
    "\"\"\"\n",
    "# select trump 2017 speech\n",
    "x = df.loc[df[\"year\"] == \"2017\"]\n",
    "tokens_2017 = x[\"tokens\"].tolist()[0]\n",
    "print(\"Total Tokens: \" + str(len(tokens_2017)))\n",
    "print(\"Unique Tokens: \" + str(len(set(tokens_2017))))\n",
    "\n",
    "# create specified number of sequences from a list of tokens\n",
    "def create_sequences(list_of_tokens, len_of_seq):\n",
    "    length = len_of_seq + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(list_of_tokens)):\n",
    "        seq = list_of_tokens[i-length:i]\n",
    "        line = \" \".join(seq)\n",
    "        sequences.append(line)\n",
    "    return sequences\n",
    "print(\"Total Sequences: \" + str(len(x)))\n",
    "\n",
    "# save tokens to file one per line\n",
    "def save_doc(lines, filename):\n",
    "    text = \"\\n\".join(lines)\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "trump_sequences = create_sequences(tokens_2017, 50)\n",
    "trump = \"trump_2017_sequences.txt\"\n",
    "save_doc(trump_sequences, trump)\n",
    "\n",
    "# load sequences into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "in_filename = \"trump_2017_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split(\"\\n\")\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = np.asarray(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "\"\"\"\n",
    "vocabulary_size = size of vocabulary\n",
    "num_of_layers = size of embedding vector space\n",
    "len_of_sequence = length of sequences\n",
    "mem_cells = LSTM number of memory cells\n",
    "num_connect_neurons = number of neurons to connect with memory cells\n",
    "\"\"\"\n",
    "def model_lang(vocabulary_size, num_of_layers, len_of_sequence, mem_cells, num_connect_neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, num_of_layers, input_length=len_of_sequence))\n",
    "    model.add(LSTM(mem_cells, return_sequences=True))\n",
    "    model.add(LSTM(mem_cells))\n",
    "    model.add(Dense(num_connect_neurons, activation=\"relu\"))\n",
    "    model.add(Dense(vocabulary_size, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "m1 = model_lang(vocab_size, 50, seq_length, 100, 100)\n",
    "print(m1.summary())\n",
    "\n",
    "m2 = model_lang(vocab_size, 300, seq_length, 300, 250)\n",
    "print(m2.summary())\n",
    "\n",
    "def compile_model(model):\n",
    "    m=model\n",
    "    m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# compile model\n",
    "m2_comp = compile_model(m2)\n",
    "\n",
    "# fit model\n",
    "m2_comp.fit(X, y, batch_size=100, epochs=100)\n",
    "m2.save(\"m2.h5\")\n",
    "\n",
    "# save model to file\n",
    "model.save(\"model.h5\")\n",
    "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict a sequence of words from a given language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "\n",
    "        # make the text consist of integers instead of words\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # pad and create sme length sequences\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    \n",
    "        # predicted probabilities for words/sequences\n",
    "        probs = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # predicted words are keyed to index\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == probs:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "    # append words to the result and return a string\n",
    "    in_textext += ' ' + out_word\n",
    "    result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this checks how similar one predicted text from the next sequence of words. \n",
    "\n",
    "cosines_2 = []\n",
    "i_2 = 0\n",
    "length = len(lines) - 3\n",
    "while i_2 < length:\n",
    "    for index in range(len(lines)):\n",
    "        combined = list(set(lines[index] + lines[index+1] + lines[index+2]))\n",
    "        train_text = \" \".join(combined)\n",
    "        test_text = lines[index+3]\n",
    "        X_train = generate_seq(model, tokenizer, seq_length, train_text, len(test_text))\n",
    "        sim = similar_to_which(X_train, test_text)[0][1]\n",
    "        print(i_2)\n",
    "        cosines_2.append(sim)\n",
    "        print(np.mean(cosines_2))\n",
    "        i_2+=1\n",
    "        \n",
    "        ##average cosine similarity is 0.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Markov Chain Dictionary and Prediction\n",
    "\"\"\"\n",
    "\n",
    "president = df.loc[df[\"year\"] == \"2017\"]\n",
    "corpus = president[\"speech\"].tolist()\n",
    "corpus = \" \".join(corpus)\n",
    "corpus = corpus.replace(\"\\n\", \" \")\n",
    "corpus_lower = corpus.lower()\n",
    "corpus = re.split('[^a-zA-Z0-9]', corpus)\n",
    "clean = list(filter(None,corpus))\n",
    "\n",
    "\n",
    "# creating bigrams from the split list in trump 2017 speech\n",
    "def make_pairs(corp):\n",
    "    for i in range(len(corp)-1):\n",
    "        yield (corp[i], corp[i+1])\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Creates a dictionary with the keys as every single word and the values as the \n",
    "adjacent words to the particular word. \n",
    "\"\"\"\n",
    "    \n",
    "def markov_dictionary(data_list):\n",
    "\n",
    "    # read in data_list called by the make_pairs function\n",
    "    pairs = make_pairs(data_list)\n",
    "    \n",
    "    # initiate an empty dictionary for appending\n",
    "    word_dictionary = {}\n",
    "    for word1, word2 in pairs:\n",
    "        if word1 in word_dictionary.keys():\n",
    "            word_dictionary[word1].append(word2)\n",
    "        else:\n",
    "            word_dictionary[word1] = [word2]\n",
    "    \n",
    "    return(word_dictionary)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Predicts from a start word a string of however long thats specific and returns a string of words. \n",
    "\"\"\"    \n",
    "\n",
    "def markov_predict(word_dictionary, start_word, length):\n",
    "    \n",
    "    \n",
    "    first_word = start_word\n",
    "    #first_word = start_word.capitalize()\n",
    "    #first_word = np.random.choice(data_list)\n",
    "    \n",
    "    #while first_word.islower():\n",
    "    #    first_word = np.random.choice(word_dictionary)\n",
    "    \n",
    "    chain = [first_word]\n",
    "    \n",
    "    n = length\n",
    "    \n",
    "    for i in range(n):\n",
    "        chain.append(np.random.choice(word_dictionary[chain[-1]]))\n",
    "    \n",
    "    output_string = \" \".join(chain)\n",
    "    \n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "attempting to create some sort of validation for markov chain.\n",
    "the model outputs the accuracy as it goes.\n",
    "\"\"\"\n",
    "\n",
    "def similar(a,b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "    \n",
    "d = markov_dictionary(clean)\n",
    "\n",
    "random_algo = random.sample(clean, 50)\n",
    "    \n",
    "train_text = lines[0]\n",
    "test_text = lines[1]\n",
    "X_model = generate_seq(model, tokenizer, seq_length, train_text, len(test_text))\n",
    "mark = markov_predict(d, test_text.split(\" \")[0],49)\n",
    "\n",
    "print(similar_to_which(test_text, X_model))\n",
    "print(similar_to_which(test_text, mark))\n",
    "print(similar_to_which(\" \".join(random_algo), test_text))\n",
    "\n",
    "\n",
    "num_k_folds = 5\n",
    "\n",
    "k_list = [clean[0:1019], clean[1019:2038], clean[2038:3057], clean[3057:4076], clean[4076:5095]]\n",
    "\n",
    "sims_markov=[]\n",
    "sims_random=[]\n",
    "for ind in range(len(k_list)):\n",
    "    a = k_list.copy()\n",
    "    del a[ind]\n",
    "    test_string = str(k_list[ind])\n",
    "    train_list = list(itertools.chain.from_iterable(a))\n",
    "    print(ind)\n",
    "\n",
    "    \n",
    "    m_dict = markov_dictionary(train_list)\n",
    "    \n",
    "    first_word = k_list[ind][0]\n",
    "    \n",
    "    i=0\n",
    "    while first_word not in m_dict.keys():\n",
    "        i+=1\n",
    "        first_word=k_list[ind][i]\n",
    "        \n",
    "    \n",
    "    train_predict = markov_predict(m_dict, first_word, 1018).lower()\n",
    "    \n",
    "    list_markov = train_predict.split(\" \")\n",
    "    \n",
    "    counts_markov = Counter(list_markov)\n",
    "    counts_test = Counter(test_string.lower().split(\" \"))\n",
    "    \n",
    "    common_markov = counts_markov.most_common()\n",
    "    common_test = counts_test.most_common()\n",
    "    \n",
    "    #print(counts_markov)\n",
    "    #print(counts_test)\n",
    "    \n",
    "    print(len(counts_markov))\n",
    "    print(len(counts_test))\n",
    "    \n",
    "    count = 0\n",
    "    if len(common_markov) > len(common_test):\n",
    "        total = len(common_test)\n",
    "        for k in range(total):\n",
    "            if common_test[j][0] == common_markov[j][0]:\n",
    "                count += 1\n",
    "    else:\n",
    "        total = len(common_markov)\n",
    "        for j in range(total):\n",
    "            if common_test[j][0] == common_markov[j][0]:\n",
    "                count += 1\n",
    "    print(count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    similarity = similar(train_predict, test_string)\n",
    "    \n",
    "    ran = random.sample(train_list,1019)\n",
    "    \n",
    "    \n",
    "    sims_random.append(similar(\" \".join(ran), test_string))\n",
    "    \n",
    "    \n",
    "    sims_markov.append(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
